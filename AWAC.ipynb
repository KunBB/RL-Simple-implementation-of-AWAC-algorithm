{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AWAC.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nsattiraju/RL-Simple-implementation-of-AWAC-algorithm/blob/main/AWAC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5f0sQWNslfb",
        "outputId": "219c2927-3bc1-4f11-8350-43afc2ab5b97"
      },
      "source": [
        "!pip install --upgrade gspread"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: gspread in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied, skipping upgrade: requests>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from gspread) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from gspread) (0.4.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from gspread) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.2.1->gspread) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (50.3.2)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.12.0->gspread) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=1.12.0->gspread) (0.4.8)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0GD-WBFyEgu",
        "outputId": "6ce59702-d718-4d25-8dd1-fb01490df9d8"
      },
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym) (1.18.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python-opengl is already the newest version (3.1.0+dfsg-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "xvfb is already the newest version (2:1.19.6-1ubuntu4.8).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTeF9JZxPuDg",
        "outputId": "999b23bd-ed1b-40d5-aa65-5681aa25aefd"
      },
      "source": [
        "!pip install gym[Box2D]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[Box2D] in /usr/local/lib/python3.6/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.5.0)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (1.18.5)\n",
            "Requirement already satisfied: box2d-py~=2.3.5; extra == \"box2d\" in /usr/local/lib/python3.6/dist-packages (from gym[Box2D]) (2.3.8)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[Box2D]) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKNPzwrsRPuv"
      },
      "source": [
        "For rendering the environment, I need pyvirtualdisplay"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QD8xSihRsc2",
        "outputId": "f58c4497-dc96-49a7-8de1-0f4e956cac48"
      },
      "source": [
        "!pip install pyvirtualdisplay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyvirtualdisplay in /usr/local/lib/python3.6/dist-packages (1.3.2)\n",
            "Requirement already satisfied: EasyProcess in /usr/local/lib/python3.6/dist-packages (from pyvirtualdisplay) (0.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kitV4lxxR4NF",
        "outputId": "d04967c9-49b5-47f8-8f41-5367932e6764"
      },
      "source": [
        "!pip install piglet"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: piglet in /usr/local/lib/python3.6/dist-packages (1.0.0)\n",
            "Requirement already satisfied: piglet-templates in /usr/local/lib/python3.6/dist-packages (from piglet) (1.1.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.1.1)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: attrs in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (20.3.0)\n",
            "Requirement already satisfied: Parsley in /usr/local/lib/python3.6/dist-packages (from piglet-templates->piglet) (1.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (0.36.1)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D21P5VcHSj-Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKmLmRiwSvcW"
      },
      "source": [
        "To activate virtual display we need to run a script once for training an agent, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TMx3iugYRzDc",
        "outputId": "eea6104d-5d60-4efa-d82f-27f28173ca86"
      },
      "source": [
        "from pyvirtualdisplay import Display\n",
        "\n",
        "display=Display(visible=0,size=(1400,900))\n",
        "display.start()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7fe9f3c79588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dz8Web8DTzdo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXi330EFTLt7"
      },
      "source": [
        "make our environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCbWXCQ1SZ-o",
        "outputId": "a1c226a6-6d8b-4073-df7b-7a4b81005601"
      },
      "source": [
        "import gym\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "env=gym.make('LunarLander-v2')\n",
        "env.reset()\n",
        "actions=env.action_space\n",
        "print(actions.n)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZaBea9nGadz",
        "outputId": "0044d0db-64be-4c7b-e479-2e4a5c44976f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-ZB7ksvAKK1"
      },
      "source": [
        "from collections import deque\r\n",
        "import random\r\n",
        "import csv\r\n",
        "\r\n",
        "class ReplayBuffer(object):\r\n",
        "    def __init__(self,buffer_size):\r\n",
        "        self.buffer_size=buffer_size\r\n",
        "        self.num_experiences=0\r\n",
        "        self.buffer=deque()\r\n",
        "    \r\n",
        "    def getBatch(self,batch_size):\r\n",
        "        #Randomly sample batch_size examples\r\n",
        "        if (self.num_experiences<batch_size):\r\n",
        "            return random.sample(self.buffer,self.num_experiences)\r\n",
        "        else:\r\n",
        "            return random.sample(self.buffer,batch_size)\r\n",
        "    def size(self):\r\n",
        "        return self.buffer_size\r\n",
        "    def add(self,state,action,reward,state_,done):\r\n",
        "        experience=(state,action,reward,state_,done)\r\n",
        "        with open('/content/drive/My Drive/AWAC_RL_ECE6504/AWAC_Implementation/dataset_AC.csv', 'a+') as f:\r\n",
        "            header=['State','Action','Reward','Next_State','Done']\r\n",
        "            writer=csv.DictWriter(f, fieldnames = header)\r\n",
        "            writer.writeheader()\r\n",
        "            writer.writerow({'State':state,\r\n",
        "                             'Action':action,\r\n",
        "                             'Reward':reward,\r\n",
        "                             'Next_State':state_,\r\n",
        "                             'Done':done})\r\n",
        "            f.close\r\n",
        "            #f.write(experience)\r\n",
        "        if (self.num_experiences<self.buffer_size):\r\n",
        "            self.buffer.append(experience)\r\n",
        "            self.num_experiences+=1            \r\n",
        "        else:\r\n",
        "            self.buffer.popleft()\r\n",
        "            self.buffer.append(experience)\r\n",
        "    def count(self):\r\n",
        "        return self.num_experiences\r\n",
        "    def erase(self):\r\n",
        "        self.buffer=deque()\r\n",
        "        self.num_experiences=0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFdk5AUkC9o-"
      },
      "source": [
        "To plot curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lNY4ujyNCopC"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def plot_learning_curve(x, scores, figure_file):\r\n",
        "    running_avg = np.zeros(len(scores))\r\n",
        "    for i in range(len(running_avg)):\r\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\r\n",
        "    plt.plot(x, running_avg)\r\n",
        "    plt.title('Running average of previous 100 scores')\r\n",
        "    #plt.savefig(figure_file)\r\n",
        "    images_dir = '/content/drive/My Drive/AWAC_RL_ECE6504/AWAC_Implementation/Images'\r\n",
        "    plt.savefig(f\"{figure_file}/abc.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MNEumyW9EJfj"
      },
      "source": [
        "New Actor Critic Method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FmczxBDEIeC"
      },
      "source": [
        "import numpy as np\r\n",
        "import torch as T\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "\r\n",
        "class ActorCriticNetwork(nn.Module):\r\n",
        "    def __init__(self, lr, input_dims, n_actions, fc1_dims=256, fc2_dims=256):\r\n",
        "        super(ActorCriticNetwork, self).__init__()\r\n",
        "        self.fc1 = nn.Linear(*input_dims, fc1_dims)\r\n",
        "        self.fc2 = nn.Linear(fc1_dims, fc2_dims)\r\n",
        "        self.pi = nn.Linear(fc2_dims, n_actions)\r\n",
        "        self.v = nn.Linear(fc2_dims, 1)\r\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=lr)\r\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\r\n",
        "        self.to(self.device)\r\n",
        "\r\n",
        "    def forward(self, state):\r\n",
        "        x = F.relu(self.fc1(state))\r\n",
        "        x = F.relu(self.fc2(x))\r\n",
        "        pi = self.pi(x)\r\n",
        "        v = self.v(x)\r\n",
        "\r\n",
        "        return (pi, v)\r\n",
        "\r\n",
        "class Agent():\r\n",
        "    def __init__(self, lr, input_dims, fc1_dims, fc2_dims, n_actions, \r\n",
        "                 gamma=0.99):\r\n",
        "        self.gamma = gamma\r\n",
        "        self.lr = lr\r\n",
        "        self.fc1_dims = fc1_dims\r\n",
        "        self.fc2_dims = fc2_dims\r\n",
        "        self.actor_critic = ActorCriticNetwork(lr, input_dims, n_actions, \r\n",
        "                                               fc1_dims, fc2_dims)\r\n",
        "        self.log_prob = None\r\n",
        "\r\n",
        "    def choose_action(self, observation):\r\n",
        "        state = T.tensor([observation], dtype=T.float).to(self.actor_critic.device)\r\n",
        "        probabilities, _ = self.actor_critic.forward(state)\r\n",
        "        probabilities = F.softmax(probabilities, dim=1)\r\n",
        "        action_probs = T.distributions.Categorical(probabilities)\r\n",
        "        action = action_probs.sample()\r\n",
        "        log_prob = action_probs.log_prob(action)\r\n",
        "        self.log_prob = log_prob\r\n",
        "\r\n",
        "        return action.item()\r\n",
        "\r\n",
        "    def learn(self, state, reward, state_, done):\r\n",
        "        self.actor_critic.optimizer.zero_grad()\r\n",
        "\r\n",
        "        state = T.tensor([state], dtype=T.float).to(self.actor_critic.device)\r\n",
        "        state_ = T.tensor([state_], dtype=T.float).to(self.actor_critic.device)\r\n",
        "        reward = T.tensor(reward, dtype=T.float).to(self.actor_critic.device)\r\n",
        "\r\n",
        "        _, critic_value = self.actor_critic.forward(state)\r\n",
        "        _, critic_value_ = self.actor_critic.forward(state_)\r\n",
        "\r\n",
        "        delta = reward + self.gamma*critic_value_*(1-int(done)) - critic_value\r\n",
        "\r\n",
        "        actor_loss = -self.log_prob*delta\r\n",
        "        critic_loss = delta**2\r\n",
        "\r\n",
        "        (actor_loss + critic_loss).backward()\r\n",
        "        self.actor_critic.optimizer.step()\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQgDds0DqbN"
      },
      "source": [
        "New main method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "id": "-PKY5pieCafU",
        "outputId": "9d9cc03d-4801-4dc5-92e2-7f6dfd3a2936"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "#from actor_critic_torch import Agent\r\n",
        "#from utils import plot_learning_curve\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    env = gym.make('LunarLander-v2')\r\n",
        "    buff=ReplayBuffer(10000)\r\n",
        "    agent = Agent(gamma=0.99, lr=5e-6, input_dims=[8], n_actions=4,\r\n",
        "                  fc1_dims=2048, fc2_dims=1536)\r\n",
        "    n_games = 3000\r\n",
        "\r\n",
        "    fname = 'ACTOR_CRITIC_' + 'lunar_lander_' + str(agent.fc1_dims) + \\\r\n",
        "            '_fc1_dims_' + str(agent.fc2_dims) + '_fc2_dims_lr' + str(agent.lr) +\\\r\n",
        "            '_' + str(n_games) + 'games'\r\n",
        "    figure_file =  fname + '.png'\r\n",
        "\r\n",
        "    scores = []\r\n",
        "    for i in range(n_games):\r\n",
        "        done = False\r\n",
        "        observation = env.reset()\r\n",
        "        score = 0\r\n",
        "        while not done:\r\n",
        "            action = agent.choose_action(observation)\r\n",
        "            observation_, reward, done, info = env.step(action)\r\n",
        "            #add to our file\r\n",
        "            buff.add(observation,action,reward, observation_,done)\r\n",
        "            score += reward\r\n",
        "            agent.learn(observation, reward, observation_, done)\r\n",
        "            observation = observation_\r\n",
        "        scores.append(score)\r\n",
        "\r\n",
        "        avg_score = np.mean(scores[-100:])\r\n",
        "        print('episode ', i, 'score %.1f' % score,\r\n",
        "                'average score %.1f' % avg_score)\r\n",
        "\r\n",
        "    x = [i+1 for i in range(n_games)]\r\n",
        "    plot_learning_curve(x, scores, figure_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "episode  0 score -256.1 average score -256.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-53e70b0c4f7a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#add to our file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mbuff\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mscore\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobservation_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-42147e0b4522>\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, state, action, reward, state_, done)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mexperience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/My Drive/AWAC_RL_ECE6504/AWAC_Implementation/dataset_AC.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'State'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Action'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Reward'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Next_State'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Done'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcsv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDictWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfieldnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/_bootlocale.py\u001b[0m in \u001b[0;36mgetpreferredencoding\u001b[0;34m(do_setlocale)\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlocale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0;32mdef\u001b[0m \u001b[0mgetpreferredencoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo_setlocale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdo_setlocale\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnl_langinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_locale\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCODESET\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VBM9SO4Al4E"
      },
      "source": [
        "Rough work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKrJ3J_qAo79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fa370f9d-d5df-472d-f45d-22ef123bc7d9"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXFVrwKcAwNC"
      },
      "source": [
        "from google.colab import auth\r\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oo41T0fngjtA"
      },
      "source": [
        "To read back from the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOlodnX_aYl-"
      },
      "source": [
        "Offline dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gIsCBCZbh647",
        "outputId": "feb251b4-8a49-45e5-ccfd-05ce8e4d6280"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oniL7UEH0s1E"
      },
      "source": [
        "\r\n",
        "from google.colab import auth\r\n",
        "auth.authenticate_user()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XhUApKQXMRqp"
      },
      "source": [
        "from keras.utils.np_utils import to_categorical\r\n",
        "def one_hot(a, num_classes):  \r\n",
        "    categorical_labels = to_categorical(a, num_classes)\r\n",
        "    return categorical_labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AnhNmsgzso70"
      },
      "source": [
        "import gspread\r\n",
        "from oauth2client.client import GoogleCredentials\r\n",
        "import pandas as pd\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "def getStates(df):\r\n",
        "\r\n",
        "    a=(df['State'].iloc[1:]).to_numpy()\r\n",
        "    b=[]\r\n",
        "    for i in a:\r\n",
        "        i=i.strip('[]')\r\n",
        "        i=i.replace('\\n','')\r\n",
        "        i=i.replace('\\t','')\r\n",
        "        l=[]\r\n",
        "        for t in i.split():\r\n",
        "            try:\r\n",
        "                l.append(float(t))\r\n",
        "            except ValueError:\r\n",
        "                pass\r\n",
        "    \r\n",
        "        b.append(l)\r\n",
        "    states=np.asarray(b,dtype=float)\r\n",
        "    return states\r\n",
        "\r\n",
        "def getNextStates(df):\r\n",
        "\r\n",
        "    c=(df['Next_State'].iloc[1:]).to_numpy()\r\n",
        "    d=[]\r\n",
        "    for i in c:\r\n",
        "        i=i.strip('[]')\r\n",
        "        i=i.replace('\\n','')\r\n",
        "        i=i.replace('\\t','')\r\n",
        "        l=[]\r\n",
        "        for t in i.split():\r\n",
        "            try:\r\n",
        "                l.append(float(t))\r\n",
        "            except ValueError:\r\n",
        "                pass\r\n",
        "    \r\n",
        "        d.append(l)\r\n",
        "    next_states=np.asarray(d,dtype=float)\r\n",
        "    return next_states\r\n",
        "\r\n",
        "def getRewards(df):\r\n",
        "    current_array=(df['Reward'].iloc[1:]).to_numpy()\r\n",
        "    a = [float(numeric_string) for numeric_string in current_array]\r\n",
        "    rewards=np.asarray(a,dtype=float)\r\n",
        "    return rewards\r\n",
        "\r\n",
        "def getActions(df):\r\n",
        "    current_array=(df['Action'].iloc[1:]).to_numpy()\r\n",
        "    a = [int(numeric_string) for numeric_string in current_array]\r\n",
        "    actions=np.asarray(a,dtype=int)\r\n",
        "    return actions\r\n",
        "\r\n",
        "def getTerminals(df):\r\n",
        "    a=[]\r\n",
        "    for boolean_value in current_array:\r\n",
        "        if (boolean_value=='TRUE'):\r\n",
        "            boolean_value=True        \r\n",
        "        else:\r\n",
        "            boolean_value=False\r\n",
        "        a.append(boolean_value)\r\n",
        "\r\n",
        "def offline_dataset():\r\n",
        "    gc = gspread.authorize(GoogleCredentials.get_application_default())\r\n",
        "    worksheet = gc.open('dataset_AC').sheet1\r\n",
        "    rows = worksheet.get_all_values()\r\n",
        "    df=pd.DataFrame.from_records(rows)\r\n",
        "    df.rename(columns={0: 'State', 1: 'Action',2:'Reward',3:'Next_State',4:'Done'}, inplace=True)\r\n",
        "    new_header = df.iloc[0] #grab the first row for the header\r\n",
        "    df = df[0:] #take the data less the header row\r\n",
        "    df.columns = new_header #set the header row as the df header\r\n",
        "    df.reset_index(drop=True)\r\n",
        "\r\n",
        "    D_states=getStates(df)\r\n",
        "    D_actions=getActions(df)\r\n",
        "    D_rewards=getRewards(df)\r\n",
        "    D_nextStates=getNextStates(df)\r\n",
        "    D_done=getTerminals(df)\r\n",
        "    D_length=len(df)\r\n",
        "    \r\n",
        "    return D_states,D_actions,D_rewards,D_nextStates,D_done,D_length\r\n",
        "    \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_f7ToDre7EeO"
      },
      "source": [
        "Replay Buffer AWAC\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNQaDC2x9O-S"
      },
      "source": [
        "import numpy as np\r\n",
        "\r\n",
        "class ReplayBuffer_AWAC():\r\n",
        "    def __init__(self, input_shape, n_actions):\r\n",
        "        D_states,D_actions,D_rewards,D_nextStates,D_done,D_length=offline_dataset()\r\n",
        "        self.mem_size = D_length-1\r\n",
        "        self.state_memory = D_states\r\n",
        "        self.new_state_memory = D_nextStates\r\n",
        "        self.action_memory = D_actions\r\n",
        "        self.reward_memory = D_rewards\r\n",
        "        self.terminal_memory = D_done\r\n",
        "        \r\n",
        "       \r\n",
        "    def store_transition(self, state, action, reward, state_, done):\r\n",
        "        index = len(Dataset)-1\r\n",
        "        self.state_memory[index] = state\r\n",
        "        self.action_memory[index] = action\r\n",
        "        self.reward_memory[index] = reward\r\n",
        "        self.new_state_memory[index] = state_\r\n",
        "        self.terminal_memory[index] = done\r\n",
        "\r\n",
        "        #self.mem_cntr += 1\r\n",
        "\r\n",
        "    def sample_buffer(self, batch_size):\r\n",
        "        #max_mem = min(self.mem_cntr, self.mem_size)\r\n",
        "\r\n",
        "        batch = np.random.choice(self.mem_size, batch_size)\r\n",
        "\r\n",
        "        states = self.state_memory[batch]\r\n",
        "        actions = self.action_memory[batch]\r\n",
        "        rewards = self.reward_memory[batch]\r\n",
        "        states_ = self.new_state_memory[batch]\r\n",
        "        dones = self.terminal_memory[batch]\r\n",
        "\r\n",
        "        return states, actions, rewards, states_, dones"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgMtNlaPPUBc"
      },
      "source": [
        "Initialize a critic network(Q phi) and actor network (pi)\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SxUrDi4PraN"
      },
      "source": [
        "import os\r\n",
        "import torch as T\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class QNetwork_Q_phi(nn.Module):\r\n",
        "    def __init__(self,beta,input_dims,fc1_dims,fc2_dims,n_actions,name,checkpoint_dir='/content/drive/My Drive/AWAC_RL_ECE6504/AWAC_Implementation/'):# have to provide the directory link here\r\n",
        "        super(QNetwork_Q_phi,self).__init__()\r\n",
        "        self.input_dims=input_dims\r\n",
        "        self.fc1_dims=fc1_dims\r\n",
        "        self.fc2_dims=fc2_dims\r\n",
        "        self.n_actions=n_actions\r\n",
        "        self.name=name\r\n",
        "        self.checkpoint_dir=checkpoint_dir\r\n",
        "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name+'_AWAC')\r\n",
        "\r\n",
        "        self.fc1 = nn.Linear(*self.input_dims, self.fc1_dims)\r\n",
        "        self.fc2 = nn.Linear(self.fc1_dims, self.fc2_dims)\r\n",
        "        self.Q = nn.Linear(fc2_dims, n_actions)\r\n",
        "        self.v = nn.Linear(fc2_dims, 1)\r\n",
        "\r\n",
        "        self.optimizer = optim.Adam(self.parameters(), lr=beta)\r\n",
        "        self.device = T.device('cuda:0' if T.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "        self.to(self.device)\r\n",
        "\r\n",
        "    def forward(self,state,action):\r\n",
        "        #print('state shape',state.shape)\r\n",
        "        #print('action shape',action.shape)\r\n",
        "        #k=T.cat([state,action],dim=1)\r\n",
        "        #print(k.shape)\r\n",
        "    \r\n",
        "        Q_action_value = F.relu(self.fc1(state))\r\n",
        "        Q_action_value = F.relu(self.fc2(Q_action_value))\r\n",
        "        Q = self.Q(Q_action_value)\r\n",
        "        V = self.v(Q_action_value)\r\n",
        "\r\n",
        "        return Q,V\r\n",
        "        \r\n",
        "        \r\n",
        "    \r\n",
        "    def save_checkpoint(self):\r\n",
        "        print('...Saving checkpoint...')\r\n",
        "        T.save(self.state_dict(),self.checkpoint_file)\r\n",
        "    \r\n",
        "    def load_checkpoint(self):\r\n",
        "        print('...loading checkpoint...')\r\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\r\n",
        "\r\n",
        "class PolicyNetwork_pi(nn.Module):\r\n",
        "    def __init__(self,alpha,input_dims,fc1_dims,fc2_dims,n_actions,name,checkpoint_dir='/content/drive/My Drive/AWAC_RL_ECE6504/AWAC_Implementation/'):\r\n",
        "        super(PolicyNetwork_pi,self).__init__()\r\n",
        "        self.input_dims=input_dims\r\n",
        "        self.fc1_dims=fc1_dims\r\n",
        "        self.fc2_dims=fc2_dims\r\n",
        "        self.n_actions=n_actions\r\n",
        "        #print('policy network n_actions',n_actions)\r\n",
        "        self.name=name\r\n",
        "        self.checkpoint_dir=checkpoint_dir\r\n",
        "        self.checkpoint_file=os.path.join(self.checkpoint_dir,name+'_AWAC')\r\n",
        "\r\n",
        "        self.fc1=nn.Linear(*self.input_dims,self.fc1_dims)\r\n",
        "        self.fc2=nn.Linear(self.fc1_dims,self.fc2_dims)\r\n",
        "        self.pi=nn.Linear(self.fc2_dims,n_actions)\r\n",
        "\r\n",
        "        self.optimizer=optim.Adam(self.parameters(),lr=alpha)\r\n",
        "        self.device=T.device('cuda:0' if T.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "        self.to(self.device)\r\n",
        "\r\n",
        "    def forward(self,state):\r\n",
        "        prob =self.fc1(state)\r\n",
        "        prob=F.relu(prob)\r\n",
        "        prob=self.fc2(prob)\r\n",
        "        prob=F.relu(prob)\r\n",
        "\r\n",
        "        prob=T.tanh(self.pi(prob)) # if action is > +/- 1 then multiply by max action\r\n",
        "        return prob\r\n",
        "    \r\n",
        "    def save_checkpoint(self):\r\n",
        "        print('...saving checkpoint...')\r\n",
        "        T.save(self.state_dict(),self.checkpoint_file)\r\n",
        "\r\n",
        "    def load_checkpoint(self):\r\n",
        "        print('...loading checkpoint...')\r\n",
        "        self.load_state_dict(T.load(self.checkpoint_file))\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxXTFtTh1AcZ"
      },
      "source": [
        "Agent AWAC"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BC0k3hTGvkVp"
      },
      "source": [
        "import os\r\n",
        "import torch as T\r\n",
        "import torch.nn.functional as F\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "class Agent_AWAC():\r\n",
        "    #def __init__(self,alpha=5e-6,beta=5e-5,, input_dims=env.observation_space.shape, n_actions=env.action_space.n,gamma=0.99,fc1_dims=400, fc2_dims=300,batch_size)\r\n",
        "    def __init__(self,alpha,beta,input_dims,n_actions,gamma,layer1_size,layer2_size,batch_size):\r\n",
        "        \r\n",
        "        self.alpha=alpha\r\n",
        "        self.beta=beta\r\n",
        "        self.input_dims=input_dims\r\n",
        "        self.n_actions=n_actions\r\n",
        "        self.gamma=gamma   \r\n",
        "        self.layer1_size=layer1_size\r\n",
        "        self.layer2_size=layer2_size \r\n",
        "        self.memory=ReplayBuffer_AWAC(input_dims, n_actions)\r\n",
        "        self.batch_size=batch_size\r\n",
        "        self.n_actions=n_actions\r\n",
        "        self.log_prob = None\r\n",
        "        \r\n",
        "        # the ones using off policy data\r\n",
        "        self.actor_beta=PolicyNetwork_pi(alpha,input_dims,layer1_size,layer2_size,n_actions=n_actions, name='Actor Pi Beta')\r\n",
        "        self.actor_theta=PolicyNetwork_pi(alpha,input_dims,layer1_size,layer2_size,n_actions=n_actions, name='Actor Pi Theta')\r\n",
        "\r\n",
        "        self.critic=QNetwork_Q_phi(beta, input_dims,layer1_size,layer2_size,n_actions=n_actions,name='Critic Q')\r\n",
        "\r\n",
        "        self.target_actor_beta=PolicyNetwork_pi(alpha,input_dims,layer1_size,layer2_size,n_actions=n_actions,name='Target Pi Beta')\r\n",
        "        #self.target_actor_theta=PolicyNetwork_pi(alpha,input_dims,layer1_size,layer2_size,n_actions=n_actions,name='Target Pi Theta')\r\n",
        "\r\n",
        "        self.target_critic=QNetwork_Q_phi(beta,input_dims,layer1_size,layer2_size,n_actions=n_actions,name='Target Critic')\r\n",
        "\r\n",
        "    def choose_action(self,observation):\r\n",
        "        state_beta = T.tensor([observation], dtype=T.float).to(self.actor_beta.device)\r\n",
        "        probabilities_beta = self.actor_beta.forward(state_beta)\r\n",
        "        probabilities_beta = F.softmax(probabilities_beta, dim=1)\r\n",
        "        action_probs_beta = T.distributions.Categorical(probabilities_beta)\r\n",
        "\r\n",
        "        action_beta = action_probs_beta.sample()\r\n",
        "        log_prob = action_probs_beta.log_prob(action_beta)\r\n",
        "        self.log_prob = log_prob\r\n",
        "\r\n",
        "        print(action.item())\r\n",
        "\r\n",
        "        return action.item()\r\n",
        "        \r\n",
        "        \r\n",
        "\r\n",
        "    def remember(self,state,action,reward,state_,done):\r\n",
        "        self.memory.store_transition(state,action,reward,state_,done)\r\n",
        "    \r\n",
        "    \r\n",
        "    def integrand(lambda_,delta):\r\n",
        "        integrand_=np.exp((1/lambda_)*delta)\r\n",
        "        return integrand_\r\n",
        "\r\n",
        "\r\n",
        "    from scipy.integrate import quad\r\n",
        "    def Z(actions_beta,n_actions,lambda_,delta):\r\n",
        "        #since advantage can also be considered as delta/td error\r\n",
        "\r\n",
        "        s1=actions_beta[action]\r\n",
        "        s2=integrand(lambda_,delta)\r\n",
        "        \r\n",
        "        Z_s = quad((s1*s2), 0, n_actions, args=(actions_beta))\r\n",
        "        return Z_s\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "    def learn(self,n_actions,lambda_,batch_size,flag):\r\n",
        "        \r\n",
        "        #sampling from beta\r\n",
        "\r\n",
        "        states_beta,actions_beta,rewards_beta,next_states_beta,dones_beta=self.memory.sample_buffer(self.batch_size)\r\n",
        "\r\n",
        "        print(dones_beta[:3]) \r\n",
        "        self.critic.optimizer.zero_grad()\r\n",
        "\r\n",
        "        rewards_beta=T.tensor(rewards_beta,dtype=T.float).to(self.critic.device)\r\n",
        "        dones_beta=T.tensor(dones_beta,dtype=T.bool).to(self.critic.device)\r\n",
        "        #print('offline done',dones_beta)\r\n",
        "        next_states_beta=T.tensor(next_states_beta,dtype=T.float).to(self.critic.device)\r\n",
        "        states_beta=T.tensor(states_beta,dtype=T.float).to(self.critic.device)\r\n",
        "        actions_beta=T.tensor(actions_beta,dtype=T.int).to(self.critic.device)\r\n",
        "\r\n",
        "        scores=[]\r\n",
        "        \r\n",
        "        for index in range(self.batch_size):\r\n",
        "            score=0\r\n",
        "            #print('done',dones_beta[index])\r\n",
        "            while not (dones_beta[index]):\r\n",
        "                if (flag=='OFFLINE'):\r\n",
        "                    target_action=self.target_actor_beta.forward(next_states_beta[index])\r\n",
        "                    actions_beta=self.actor_beta.forward(states_beta[index])\r\n",
        "                else:\r\n",
        "                    target_action=self.target_actor_theta.forward(next_states_beta[index])\r\n",
        "                    actions_beta=self.actor_theta.forward(states_beta[index])\r\n",
        "            \r\n",
        "\r\n",
        "                Q_beta_,value_beta_=self.target_critic.forward(next_states_beta[index],target_action)\r\n",
        "                Q_beta,value_beta=self.critic.forward(states_beta[index],actions_beta[index])\r\n",
        "\r\n",
        "                print('rewards_beta[index]',rewards_beta[index])\r\n",
        "                print('value_beta_',value_beta_)\r\n",
        "                print('int(dones_beta[index])',int(dones_beta[index]))\r\n",
        "                print('value_beta',value_beta)\r\n",
        "            \r\n",
        "                delta_beta = rewards_beta[index] + self.gamma*value_beta_*(1-int(dones_beta[index])) - value_beta\r\n",
        "                score += rewards_beta[index] + self.gamma*value_beta\r\n",
        "            \r\n",
        "            \r\n",
        "                actor_loss = -self.log_prob*delta_beta\r\n",
        "                critic_loss = delta_beta**2\r\n",
        "\r\n",
        "                actor_loss.backward()\r\n",
        "                critic_loss.backward()\r\n",
        "\r\n",
        "                self.critic.optimizer.step()\r\n",
        "                if (flag=='OFFLINE'):\r\n",
        "                    self.actor_beta.optimizer.step()\r\n",
        "                else:\r\n",
        "                    self.actor_theta.optimizer.step()\r\n",
        "\r\n",
        "                Z_beta=Z(actions_beta[index],n_actions,lambda_,delta_beta)\r\n",
        "\r\n",
        "                pi_theta=self.actor_theta.forward(states_beta[index]) #we get policy theta from input state_beta\r\n",
        "                log_pi_theta=np.log(pi_theta)\r\n",
        "\r\n",
        "                actor_theta_=log_pi_theta*(1/Z_beta)*integrand(lambda_,delta_beta)\r\n",
        "            scores.append(score)\r\n",
        "        return scores    \r\n",
        "\r\n",
        "        \r\n",
        "   \r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "\r\n",
        "        \r\n",
        "\r\n",
        "      \r\n",
        "        \r\n",
        "\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjKIN6iEIOy8"
      },
      "source": [
        "import gym\r\n",
        "env = gym.make('LunarLander-v2')\r\n",
        "input_dims=env.observation_space.shape\r\n",
        "n_actions=env.action_space\r\n",
        "print(n_actions)\r\n",
        "print(input_dims)\r\n",
        "observation=env.reset()\r\n",
        "action=env.action_space.sample()\r\n",
        "print('action',action)\r\n",
        "state_, reward, done, info =env.step(3)\r\n",
        "print(input_dims)\r\n",
        "print(state_)\r\n",
        "print(reward)\r\n",
        "print(info)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unILV-Pa6j-i"
      },
      "source": [
        "Plotting curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OKrfNZVX6jJE"
      },
      "source": [
        "import numpy as np\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "def plot_learning_curve(x, scores, figure_file):\r\n",
        "    running_avg = np.zeros(len(scores))\r\n",
        "    for i in range(len(running_avg)):\r\n",
        "        running_avg[i] = np.mean(scores[max(0, i-100):(i+1)])\r\n",
        "    plt.plot(x, running_avg)\r\n",
        "    plt.title('Running average of previous 100 scores')\r\n",
        "    plt.savefig(figure_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLkL0gRDb5QK"
      },
      "source": [
        "Main Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 522
        },
        "id": "-10P-caEb4Vr",
        "outputId": "2b0e64de-24bf-47c3-9fc7-f289192f5cf4"
      },
      "source": [
        "import gym\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "if __name__ == '__main__':\r\n",
        "    \r\n",
        "    \r\n",
        "    \r\n",
        "    env = gym.make('LunarLander-v2')\r\n",
        "    number_offline_steps=3000\r\n",
        "    n_games=5\r\n",
        "    lambda_=5e-6\r\n",
        "    input_dims=[8]\r\n",
        "    n_actions=4\r\n",
        "    gamma=0.99\r\n",
        "    layer1_size=2048\r\n",
        "    layer2_size=1536\r\n",
        "    batch_size=100\r\n",
        "    alpha=5e-6\r\n",
        "    beta=5e-5\r\n",
        "    agent = Agent_AWAC(alpha,beta,input_dims, n_actions,gamma,layer1_size, layer2_size,batch_size)\r\n",
        "    buff=ReplayBuffer_AWAC(input_dims, n_actions)\r\n",
        "    \r\n",
        "\r\n",
        "    fname = 'AWAC' + 'lunar_lander_' + str(agent.layer1_size) + \\\r\n",
        "            ':layer1_size' + str(agent.layer2_size) + ':layer2_size'+ str(agent.alpha)+'alpha for actor' + str(agent.beta)+' :beta for critic' +\\\r\n",
        "            '_' + str(n_games) + 'games'\r\n",
        "    figure_file =  fname + '.png'\r\n",
        "\r\n",
        "    scores_history = []\r\n",
        "    offline_step=0\r\n",
        "    for i in range(n_games):\r\n",
        "        if (offline_step>number_offline_steps):\r\n",
        "            Flag='ONLINE'\r\n",
        "        else:\r\n",
        "            Flag='OFFLINE'    \r\n",
        "             \r\n",
        "        scores=agent.learn(n_actions,lambda_,batch_size,Flag)\r\n",
        "        offline_step+=1\r\n",
        "\r\n",
        "        if (Flag=='ONLINE'):\r\n",
        "            observation = env.reset()\r\n",
        "            action = agent.choose_action(observation)\r\n",
        "            observation_, reward, done, info = env.step(action)\r\n",
        "            buff.remember(observation,action,reward, observation_,done)\r\n",
        "            score+=reward           \r\n",
        "        \r\n",
        "        print(np.mean(scores))         \r\n",
        "        scores_history.append(np.mean(scores))\r\n",
        "        \r\n",
        "        \r\n",
        "\r\n",
        "\r\n",
        "        #avg_score = np.mean(scores[-100:])\r\n",
        "        #print('iteration ', i, ' avg score %.1f' % np.mean(scores))\r\n",
        "                \r\n",
        "\r\n",
        "    #x = [i+1 for i in range(n_games)]\r\n",
        "    #plot_learning_curve(x, scores_history, figure_file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-b0dd5751ee91>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0mFlag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'OFFLINE'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mscores\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_actions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlambda_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mFlag\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0moffline_step\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-0b00d7592f18>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, n_actions, lambda_, batch_size, flag)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#sampling from beta\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mstates_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mactions_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrewards_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnext_states_beta\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdones_beta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdones_beta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-16-e92cc2592796>\u001b[0m in \u001b[0;36msample_buffer\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mstates_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew_state_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mdones\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal_memory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}